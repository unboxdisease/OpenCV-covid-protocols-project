{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face detector model...\n",
      "[INFO] loading face mask detector model...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tracker import *\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "from math import pow, sqrt\n",
    "def detect_and_predict_mask(frame, faceNet, maskNet):\n",
    "\t# grab the dimensions of the frame and then construct a blob\n",
    "\t# from it\n",
    "\t(h, w) = frame.shape[:2]\n",
    "\tblob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\n",
    "\t\t(104.0, 177.0, 123.0))\n",
    "\n",
    "\t# pass the blob through the network and obtain the face detections\n",
    "\tfaceNet.setInput(blob)\n",
    "\tdetections = faceNet.forward()\n",
    "\n",
    "\t# initialize our list of faces, their corresponding locations,\n",
    "\t# and the list of predictions from our face mask network\n",
    "\tfaces = []\n",
    "\tlocs = []\n",
    "\tpreds = []\n",
    "\n",
    "\t# loop over the detections\n",
    "\tfor i in range(0, detections.shape[2]):\n",
    "\t\t# extract the confidence (i.e., probability) associated with\n",
    "\t\t# the detection\n",
    "\t\tconfidence = detections[0, 0, i, 2]\n",
    "\n",
    "\t\t# filter out weak detections by ensuring the confidence is\n",
    "\t\t# greater than the minimum confidence\n",
    "\t\tif confidence > 0.8:\n",
    "\t\t\t# compute the (x, y)-coordinates of the bounding box for\n",
    "\t\t\t# the object\n",
    "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t# ensure the bounding boxes fall within the dimensions of\n",
    "\t\t\t# the frame\n",
    "\t\t\t(startX, startY) = (max(0, startX), max(0, startY))\n",
    "\t\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
    "\n",
    "\t\t\t# extract the face ROI, convert it from BGR to RGB channel\n",
    "\t\t\t# ordering, resize it to 224x224, and preprocess it\n",
    "\t\t\tface = frame[startY:endY, startX:endX]\n",
    "\t\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "\t\t\tface = cv2.resize(face, (224, 224))\n",
    "\t\t\tface = img_to_array(face)\n",
    "\t\t\tface = preprocess_input(face)\n",
    "\n",
    "\t\t\t# add the face and bounding boxes to their respective\n",
    "\t\t\t# lists\n",
    "\t\t\tfaces.append(face)\n",
    "\t\t\tlocs.append((startX, startY, endX, endY))\n",
    "\n",
    "\t# only make a predictions if at least one face was detected\n",
    "\tif len(faces) > 0:\n",
    "\t\t# for faster inference we'll make batch predictions on *all*\n",
    "\t\t# faces at the same time rather than one-by-one predictions\n",
    "\t\t# in the above `for` loop\n",
    "\t\tfaces = np.array(faces, dtype=\"float32\")\n",
    "\t\tpreds = maskNet.predict(faces, batch_size=32)\n",
    "\n",
    "\t# return a 2-tuple of the face locations and their corresponding\n",
    "\t# locations\n",
    "\treturn (locs, preds)\n",
    "\n",
    "# load our serialized face detector model from disk\n",
    "print(\"[INFO] loading face detector model...\")\n",
    "\n",
    "faceNet = cv2.dnn.readNet(\"./face_detector/deploy.prototxt\", \"./face_detector/res10_300x300_ssd_iter_140000.caffemodel\")\n",
    "\n",
    "# # load the face mask detector model from disk\n",
    "print(\"[INFO] loading face mask detector model...\")\n",
    "maskNet = load_model(\"./mask_detector.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt\"\n",
    "frozen = \"frozen_inference_graph.pb\"\n",
    "model = cv2.dnn_DetectionModel(frozen,config_file)\n",
    "tracker = EuclideanDistTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = []\n",
    "labels_file = \"labels.txt\"\n",
    "with open(\"labels.txt\",'rt') as f:\n",
    "\n",
    "    class_labels = f.read().rstrip('\\n').split('\\n')\n",
    "# print(class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dnn_Model 0x7f510035db10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.setInputSize(320,320)\n",
    "model.setInputScale(1.0/127.5)\n",
    "model.setInputMean((127.5,127.5,127.5))\n",
    "model.setInputSwapRB(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: (323, 301)}\n",
      "{8: (324, 302)}\n",
      "{8: (324, 302)}\n",
      "{8: (323, 303)}\n",
      "{8: (323, 303)}\n",
      "{8: (324, 303)}\n",
      "{8: (322, 304)}\n",
      "{8: (324, 303)}\n",
      "{8: (323, 302)}\n",
      "{8: (323, 302)}\n",
      "{8: (322, 302)}\n",
      "{8: (323, 302)}\n",
      "{8: (323, 302)}\n",
      "{8: (325, 303)}\n",
      "{8: (325, 302)}\n",
      "{8: (324, 303)}\n",
      "{8: (325, 303)}\n",
      "{8: (324, 303)}\n",
      "{8: (324, 305)}\n",
      "{8: (323, 305)}\n",
      "{8: (325, 303)}\n",
      "{8: (325, 305)}\n",
      "{8: (326, 303)}\n",
      "{8: (325, 304)}\n",
      "{8: (326, 303)}\n",
      "{8: (326, 304)}\n",
      "{8: (327, 305)}\n",
      "{8: (325, 305)}\n",
      "{8: (326, 304)}\n",
      "{8: (326, 304)}\n",
      "{8: (326, 304)}\n",
      "{8: (325, 304)}\n",
      "{8: (326, 303)}\n",
      "{8: (326, 301)}\n",
      "{8: (326, 301)}\n",
      "{8: (324, 300)}\n",
      "{8: (325, 302)}\n",
      "{8: (318, 295)}\n",
      "{8: (306, 288)}\n",
      "{8: (311, 303)}\n",
      "{8: (316, 312)}\n",
      "{10: (316, 278)}\n",
      "{10: (332, 287)}\n",
      "{10: (311, 278)}\n",
      "{10: (315, 272)}\n",
      "{10: (325, 286)}\n",
      "{10: (322, 284)}\n",
      "{10: (313, 277)}\n",
      "{10: (309, 276)}\n",
      "{10: (310, 279)}\n",
      "{10: (318, 282)}\n",
      "{10: (325, 283)}\n",
      "{10: (334, 283)}\n",
      "{10: (339, 282)}\n",
      "{10: (345, 283)}\n",
      "{10: (349, 281)}\n",
      "{10: (350, 280)}\n",
      "{10: (352, 280)}\n",
      "{10: (349, 278)}\n",
      "{10: (348, 273)}\n",
      "{10: (341, 267)}\n",
      "{10: (337, 266)}\n",
      "{10: (332, 265)}\n",
      "{10: (329, 265)}\n",
      "{10: (325, 267)}\n",
      "{10: (322, 267)}\n",
      "{10: (324, 267)}\n",
      "{10: (326, 266)}\n",
      "{10: (325, 263)}\n",
      "{10: (325, 262)}\n",
      "{10: (327, 262)}\n",
      "{10: (328, 263)}\n",
      "{10: (329, 264)}\n",
      "{10: (329, 264)}\n",
      "{10: (328, 264)}\n",
      "{10: (329, 265)}\n",
      "{10: (330, 268)}\n",
      "{10: (330, 263)}\n",
      "{10: (331, 262)}\n",
      "{10: (331, 266)}\n",
      "{10: (331, 267)}\n",
      "{10: (331, 267)}\n",
      "{10: (332, 263)}\n",
      "{10: (329, 264)}\n",
      "{10: (330, 267)}\n",
      "{10: (328, 266)}\n",
      "{10: (329, 264)}\n",
      "{10: (329, 265)}\n",
      "{10: (330, 266)}\n",
      "{10: (330, 266)}\n",
      "{10: (330, 267)}\n",
      "{10: (330, 268)}\n",
      "{10: (329, 267)}\n",
      "{10: (332, 268)}\n",
      "{10: (328, 267)}\n",
      "{10: (329, 267)}\n",
      "{10: (330, 266)}\n",
      "{10: (331, 266)}\n",
      "{10: (330, 265)}\n",
      "{10: (330, 264)}\n",
      "{10: (332, 266)}\n",
      "{10: (334, 266)}\n",
      "{10: (334, 267)}\n",
      "{10: (337, 276)}\n",
      "{10: (337, 267)}\n",
      "{10: (337, 266)}\n",
      "{10: (337, 265)}\n",
      "{10: (336, 265)}\n",
      "{10: (337, 265)}\n",
      "{10: (336, 265)}\n",
      "{10: (337, 267)}\n",
      "{11: (338, 267)}\n",
      "{11: (338, 268)}\n",
      "{11: (337, 266)}\n",
      "{11: (339, 265)}\n",
      "{11: (339, 265)}\n",
      "{11: (338, 267)}\n",
      "{11: (340, 264)}\n",
      "{11: (340, 264)}\n",
      "{11: (339, 265)}\n",
      "{11: (339, 265)}\n",
      "{11: (338, 265)}\n",
      "{11: (338, 265)}\n",
      "{11: (338, 265)}\n",
      "{11: (337, 265)}\n",
      "{11: (337, 265)}\n",
      "{11: (337, 265)}\n",
      "{11: (337, 265)}\n",
      "{11: (337, 265)}\n",
      "{11: (337, 266)}\n",
      "{11: (338, 266)}\n",
      "{11: (337, 267)}\n",
      "{11: (338, 265)}\n",
      "{11: (339, 266)}\n",
      "{11: (337, 267)}\n",
      "{11: (337, 268)}\n",
      "{11: (338, 267)}\n",
      "{11: (338, 267)}\n",
      "{11: (337, 266)}\n",
      "{11: (336, 266)}\n",
      "{11: (336, 265)}\n",
      "{11: (338, 266)}\n",
      "{11: (337, 263)}\n",
      "{11: (337, 265)}\n",
      "{11: (337, 264)}\n",
      "{11: (337, 264)}\n",
      "{11: (337, 264)}\n",
      "{11: (337, 265)}\n",
      "{11: (337, 264)}\n",
      "{11: (337, 266)}\n",
      "{11: (337, 266)}\n",
      "{11: (339, 266)}\n",
      "{11: (338, 267)}\n",
      "{11: (339, 267)}\n",
      "{11: (339, 268)}\n",
      "{11: (339, 267)}\n",
      "{11: (339, 268)}\n",
      "{11: (339, 267)}\n",
      "{11: (338, 267)}\n",
      "{11: (338, 267)}\n",
      "{11: (337, 268)}\n",
      "{11: (337, 267)}\n",
      "{11: (337, 267)}\n",
      "{11: (336, 268)}\n",
      "{11: (337, 267)}\n",
      "{11: (337, 267)}\n",
      "{11: (338, 268)}\n",
      "{11: (338, 267)}\n",
      "{11: (336, 267)}\n",
      "{11: (338, 266)}\n",
      "{11: (339, 268)}\n",
      "{11: (338, 268)}\n",
      "{11: (338, 267)}\n",
      "{11: (338, 267)}\n",
      "{11: (339, 268)}\n",
      "{11: (339, 268)}\n",
      "{11: (339, 268)}\n",
      "{11: (340, 268)}\n",
      "{11: (339, 268)}\n",
      "{11: (339, 267)}\n",
      "{11: (340, 267)}\n",
      "{11: (340, 268)}\n",
      "{11: (340, 268)}\n",
      "{11: (341, 268)}\n",
      "{11: (343, 267)}\n",
      "{11: (340, 267)}\n",
      "{11: (339, 267)}\n",
      "{11: (338, 267)}\n",
      "{11: (340, 269)}\n",
      "{11: (340, 267)}\n",
      "{11: (338, 267)}\n",
      "{11: (337, 267)}\n",
      "{11: (336, 266)}\n",
      "{11: (337, 267)}\n",
      "{11: (336, 266)}\n",
      "{11: (338, 267)}\n",
      "{11: (338, 267)}\n",
      "{11: (336, 265)}\n",
      "{11: (330, 268)}\n",
      "{11: (325, 275)}\n",
      "{11: (311, 287)}\n",
      "{11: (300, 281)}\n",
      "{11: (292, 289)}\n",
      "{11: (289, 291)}\n",
      "{11: (283, 287)}\n",
      "{11: (282, 289)}\n",
      "{11: (281, 294)}\n",
      "{11: (281, 295)}\n",
      "{11: (284, 290)}\n",
      "{11: (297, 280)}\n",
      "{11: (321, 275)}\n",
      "{11: (331, 270)}\n",
      "{11: (333, 268)}\n",
      "{11: (335, 268)}\n",
      "{11: (336, 271)}\n",
      "{11: (339, 271)}\n",
      "{11: (339, 271)}\n",
      "{11: (341, 274)}\n",
      "{11: (339, 273)}\n",
      "{11: (339, 272)}\n",
      "{11: (339, 269)}\n",
      "{11: (337, 268)}\n",
      "{11: (334, 267)}\n",
      "{11: (332, 267)}\n",
      "{11: (331, 264)}\n",
      "{11: (326, 262)}\n",
      "{11: (324, 262)}\n",
      "{11: (323, 265)}\n",
      "{11: (324, 279)}\n",
      "{11: (326, 285)}\n",
      "{11: (327, 286)}\n",
      "{11: (323, 282)}\n",
      "{11: (322, 278)}\n",
      "{11: (320, 279)}\n",
      "{11: (324, 282)}\n",
      "{11: (327, 285)}\n",
      "{11: (328, 287)}\n",
      "{11: (331, 285)}\n"
     ]
    }
   ],
   "source": [
    "cap  = cv2.VideoCapture(0)\n",
    "\n",
    "fontscale = 1\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "\n",
    "\n",
    "# Focal length\n",
    "F = 40\n",
    "\n",
    "while True : \n",
    "    \n",
    "    ret,frame = cap.read()\n",
    "    ret1,frame1 = cap.read()\n",
    "    class_index,confidence,bbox = model.detect(frame,confThreshold = 0.6)\n",
    "    frame = imutils.resize(frame1, width=400)\n",
    "    (locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\n",
    "#     print(preds)\n",
    "    detections = []\n",
    "#     print(locs)\n",
    "    for loc in locs:\n",
    "        detections.append(loc)\n",
    "    person_id = tracker.update(detections)\n",
    "    i1 = 0\n",
    "    pos_dict = dict()\n",
    "    coordinates = dict()\n",
    "    for (box, pred ) in zip(locs, preds):\n",
    "        \n",
    "        \n",
    "        # unpack the bounding box and predictions\n",
    "        (startX, startY, endX, endY) = box\n",
    "        (mask, withoutMask) = pred\n",
    "\n",
    "\t\t# determine the class label and color we'll use to draw\n",
    "\t\t# the bounding box and text\n",
    "        label = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
    "        color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
    "\t\t\t\n",
    "\t\t# include the probability in the label\n",
    "        label = \"{}\".format(label)\n",
    "\n",
    "\t\t# display the label and bounding box rectangle on the output\n",
    "\t\t# frame\n",
    "        cv2.putText(frame, label, (startX, startY - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "        \n",
    "        #distance part\n",
    "        \n",
    "        coordinates[i1] = (startX, startY, endX, endY)\n",
    "\n",
    "        # Mid point of bounding box\n",
    "        x_mid = round((startX+endX)/2,4)\n",
    "        y_mid = round((startY+endY)/2,4)\n",
    "\n",
    "        height = round(endY-startY,4)\n",
    "\n",
    "        # Distance from camera based on triangle similarity\n",
    "        distance = (165 * F)/height\n",
    "#         print(\"Distance(cm):{dist}\\n\".format(dist=distance))\n",
    "\n",
    "        # Mid-point of bounding boxes (in cm) based on triangle similarity technique\n",
    "        x_mid_cm = (x_mid * distance) / F\n",
    "        y_mid_cm = (y_mid * distance) / F\n",
    "        pos_dict[i1] = (x_mid_cm,y_mid_cm,distance)\n",
    "        i1+=1\n",
    "        \n",
    "        if (len(class_index) != 0 ):\n",
    "\n",
    "            for ci,conf,b in zip(class_index.flatten(),confidence.flatten(),bbox):\n",
    "                x1,y1,w1,h1 = b\n",
    "                if ci <= 80:\n",
    "                    cv2.putText(frame,class_labels[ci - 1],(startX, startY - 45),font,fontScale = 1,color = (0,255,255),thickness = 2)\n",
    "\n",
    "\n",
    "            \n",
    "    close_objects = set()\n",
    "    for i in pos_dict.keys():\n",
    "        for j in pos_dict.keys():\n",
    "            if i < j:\n",
    "                dist = sqrt(pow(pos_dict[i][0]-pos_dict[j][0],2) + pow(pos_dict[i][1]-pos_dict[j][1],2) + pow(pos_dict[i][2]-pos_dict[j][2],2))\n",
    "\n",
    "                # Check if distance less than 2 metres or 200 centimetres\n",
    "                if dist < 350:\n",
    "                    close_objects.add(i)\n",
    "                    close_objects.add(j)\n",
    "    \n",
    "    for i in pos_dict.keys():\n",
    "        if i in close_objects:\n",
    "            COLOR = (0,0,255)\n",
    "            stri = \"TOO CLOSE\"\n",
    "        else:\n",
    "            COLOR = (0,255,0)\n",
    "            stri = \"DISTANCING OK\"\n",
    "        (startX, startY, endX, endY) = coordinates[i]\n",
    "\n",
    "        #cv2.rectangle(frame, (startX, startY), (endX, endY), COLOR, 2)\n",
    "        y = endY + 15 \n",
    "        # Convert cms to feet\n",
    "        cv2.putText(frame, stri + ' {i} ft'.format(i=round(pos_dict[i][2]/30.48,2)), (startX, y),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, COLOR, 2)\n",
    "        \n",
    "    for pid in person_id:\n",
    "        \n",
    "        x,y,w,h,id1 = pid\n",
    "        cv2.putText(frame,\"ID : \" + str(id1),(x, y - 25),cv2.FONT_HERSHEY_SIMPLEX,0.65,(255,0,0),2)\n",
    "            \n",
    "\n",
    "    cv2.imshow(\"Kushal's Application\",frame)\n",
    "    if cv2.waitKey(2) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
